---
---
title: "FinalProject_JH_012019"
author: "Christos Antoniadis"
date: "01/2019"
output: html_document
keep_md: yes
pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE, fig.width=10, fig.height=5)
options(width=120)
rm(list=ls(all=TRUE))
# Load the Required for the Process Packages
library(readr)
library(ggplot2)
library(Rmisc)
library(caret)
library(gbm)
library(rpart)
library(e1071)
library(RColorBrewer)
library(e1071)
library(xgboost)
library(Matrix)
library(methods)
library(caret)
library(dplyr)
library(Metrics)
```
# Set a certain seed
```{r}
set.seed(15041984)
```
# Load the related Data
Data are extracted directly from the link provided
```{r}
url4train <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
url4test <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
trainInput <- read.csv(url(url4train), na.strings = c("NA","#DIV/0!",""))
testInput <- read.csv(url(url4test), na.strings = c("NA","#DIV/0!",""))
```
# Train dataset
Check for NAs and exclude them
```{r}
columnNACounts <- colSums(is.na(trainInput))             
to_exclude <- columnNACounts >= 0.5*dim(trainInput)[1]   
training <- trainInput[!to_exclude]         
```
# Test dataset
Check for NAs and exclude them
```{r}
columnNACounts <- colSums(is.na(testInput))
to_exclude <- columnNACounts >= 0.5*dim(testInput)[1] 
testing <- testInput[!to_exclude]
myDataNZV <- nearZeroVar(training, saveMetrics=TRUE)
nzvIndex<-which((myDataNZV[,4]) == "TRUE")
```
# Actions regarding zeros
```{r}
training <- training[,-nzvIndex]
testing <- testing[,-nzvIndex]
rm(myDataNZV)
rm(nzvIndex)
```
# Data Transformations & Formation
```{r}
training$classe <- factor(training$classe)
training$user_name <- factor(training$user_name)
training$cvtd_timestamp <- NULL
testing$classe <- -1
testing$user_name <- factor(testing$user_name)
testing$cvtd_timestamp <- NULL

feature.names <- names(training)
feature.names <- feature.names[-which(feature.names %in% c('X', 'classe'))]
feature.names <- feature.names[-which(feature.names %in% c('user_name', 'raw_timestamp_part_1', 'raw_timestamp_part_2', 'new_window', 'num_window'))]
feature.formula <- formula(paste('classe ~ ', paste(feature.names, collapse = ' + '), sep = ''))

dtrain_cv <- training[, c(feature.names, 'classe')]
inTrain<- createDataPartition(y = dtrain_cv$classe, p = 0.7, list = FALSE)
dtrain.matrix <- sparse.model.matrix(feature.formula, data = dtrain_cv[inTrain, ])
dtrain <- xgb.DMatrix(dtrain.matrix, label = dtrain_cv[inTrain, 'classe'])
dvalid <- xgb.DMatrix(sparse.model.matrix(feature.formula, data = dtrain_cv[-inTrain, ]),
                      label = dtrain_cv[-inTrain, 'classe'])

dtest_cv <- testing[, c(feature.names, 'classe')]
dtest <- sparse.model.matrix(feature.formula, data = dtest_cv)
```
# Cross Validation - Parameters' Specification
```{r}
numberOfClasses <- length(unique(training$classe))
n_rounds.cv <- 250
xgb.params <- list(booster = "gbtree", objective = "multi:softmax",
               num_class = numberOfClasses+1, eval_metric = 'merror',
               max_depth = 6, eta = 0.1,
               colsample_bytree = 1, subsample = 1)

bst.cv <- xgb.cv(xgb.params, dtrain, n_rounds.cv, nfold = 5, metrics = {'merror'},
                 print_every_n = 25, prediction = TRUE)
n_rounds.train <- which.min(bst.cv$evaluation_log$train_merror_mean)
n_rounds.train
```
# Train the model
Built the xgboost model
```{r}
bst_model <- xgb.train(params = xgb.params, data = dtrain, nrounds = n_rounds.train)
```
# Train Set Accuracy
```{r}
predicted <- factor(predict(bst_model, dtrain), labels = levels(training$classe))
confusionMatrix(predicted, dtrain_cv[inTrain, 'classe'])
```
# Validation dataset accuracy
```{r}
predicted <- factor(predict(bst_model, dvalid), labels = levels(training$classe))
confusionMatrix(predicted, dtrain_cv[-inTrain, 'classe'])
```
# Feature importance
```{r}
feature.importance <- xgb.importance(dimnames(dtrain.matrix)[[2]], model = bst_model)
head(feature.importance)
xgb.plot.importance(feature.importance)
```
# Predict test cases
```{r}
testCases <- factor(predict(bst_model, dtest), labels = levels(training$classe))
testCases <- as.character(testCases)
testCases
```
